##  Objetivo:
Entender como a previs茫o do pr贸ximo token funciona
sem redes neurais.

Ideia:
Contar quais palavras costumam vir depois de outras.

Aprendizado:
Mesmo sem "intelig锚ncia", padr玫es emergem.

##  escrever um script que:
recebe um texto de treino
constr贸i uma tabela:

```bash
palavra_atual -> {proxima_palavra: contagem}
```

## dado um token, prev锚 o pr贸ximo

- uma LLM prev锚 o pr贸ximo token
- tokens podem ser palavras ou n煤meros
- probabilidades surgem de repeti莽茫o
- n茫o existe "pensamento", s贸 estat铆stica

next_token_model.py ==> c茅rebro primitivo