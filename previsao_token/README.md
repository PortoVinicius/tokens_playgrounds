##  Objetivo:
Entender como a previsÃ£o do prÃ³ximo token funciona
sem redes neurais.

Ideia:
Contar quais palavras costumam vir depois de outras.

Aprendizado:
Mesmo sem "inteligÃªncia", padrÃµes emergem.

## ğŸ‘‰ escrever um script que:
recebe um texto de treino
constrÃ³i uma tabela:

```bash
palavra_atual -> {proxima_palavra: contagem}
```

## dado um token, prevÃª o prÃ³ximo

- uma LLM prevÃª o prÃ³ximo token
- tokens podem ser palavras ou nÃºmeros
- probabilidades surgem de repetiÃ§Ã£o
- nÃ£o existe "pensamento", sÃ³ estatÃ­stica

next_token_model.py ==> â€œcÃ©rebro primitivoâ€

## Pipeline real:

- tokens â†’ embeddings â†’ camadas â†’ logits â†’ softmax â†’ probabilidades

## Pipeline final (mental)

scores
 â†“
divisÃ£o por temperatura
 â†“
softmax
 â†“
filtragem (top-k / top-p)
 â†“
amostragem

## Um embedding Ã©:

um vetor de nÃºmeros em um espaÃ§o de alta dimensÃ£o

```bash

"tudo"       â†’ [ 0.2, -0.4,  0.9 ]
"bem"        â†’ [ 0.21, -0.38, 0.88 ]
"carro"      â†’ [ -0.7, 0.1, -0.2 ]
```

VocÃª jÃ¡ vÃª:

"tudo" perto de "bem"
"carro" longe

```bash
texto
 â†“
tokenizaÃ§Ã£o
 â†“
Ã­ndices
 â†“
embeddings (geometria)
 â†“
transformaÃ§Ãµes lineares
 â†“
scores
 â†“
softmax + temperatura
 â†“
amostragem
```

## Cada token vira trÃªs vetores:

Query (Q) â†’ o que eu estou procurando
Key (K) â†’ o que eu tenho
Value (V) â†’ a informaÃ§Ã£o que eu entrego